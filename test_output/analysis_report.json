{
  "source_file_name": "sample_spark_script.py",
  "analysis_timestamp": "2025-08-15T09:06:40.910766",
  "package_analysis": {
    "standard_libs": [
      {
        "import_statement": "import os",
        "purpose": "Operating system interface for file and directory operations"
      },
      {
        "import_statement": "import sys",
        "purpose": "System-specific parameters and functions"
      },
      {
        "import_statement": "from datetime import datetime, timedelta",
        "purpose": "Date and time manipulation utilities"
      },
      {
        "import_statement": "from typing import List, Dict, Any",
        "purpose": "Type hints for better code documentation and IDE support"
      }
    ],
    "third_party": [
      {
        "import_statement": "from pyspark.sql import SparkSession, DataFrame",
        "purpose": "Core PySpark SQL components for distributed data processing"
      },
      {
        "import_statement": "from pyspark.sql.functions import col, lit, when, sum as spark_sum, count, avg",
        "purpose": "PySpark SQL functions for data transformation and aggregation"
      },
      {
        "import_statement": "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType",
        "purpose": "PySpark data types for schema definition"
      }
    ],
    "custom_modules": [
      {
        "import_statement": "from utils.data_validator import validate_data_quality",
        "purpose": "Custom data validation functionality"
      },
      {
        "import_statement": "from config.spark_config import get_spark_config",
        "purpose": "Custom Spark configuration management"
      }
    ]
  },
  "function_analysis": [
    {
      "function_name": "create_spark_session",
      "dependencies": {
        "internal_functions": [],
        "external_packages": [
          "pyspark.sql.SparkSession"
        ]
      },
      "suggested_patterns": [
        "pyspark.SparkSession.builder",
        "pyspark.SparkSession.appName",
        "pyspark.SparkSession.config",
        "pyspark.SparkSession.getOrCreate"
      ]
    },
    {
      "function_name": "load_customer_data",
      "dependencies": {
        "internal_functions": [],
        "external_packages": [
          "pyspark.sql.SparkSession",
          "pyspark.sql.DataFrame",
          "pyspark.sql.types.StructType",
          "pyspark.sql.types.StructField",
          "pyspark.sql.types.StringType",
          "pyspark.sql.types.IntegerType",
          "pyspark.sql.types.DoubleType"
        ]
      },
      "suggested_patterns": [
        "pyspark.read.csv",
        "pyspark.read.option",
        "pyspark.read.schema",
        "pyspark.schema.definition"
      ]
    },
    {
      "function_name": "load_transaction_data",
      "dependencies": {
        "internal_functions": [],
        "external_packages": [
          "pyspark.sql.SparkSession",
          "pyspark.sql.DataFrame"
        ]
      },
      "suggested_patterns": [
        "pyspark.read.json",
        "pyspark.read.option"
      ]
    },
    {
      "function_name": "clean_customer_data",
      "dependencies": {
        "internal_functions": [
          "validate_data_quality"
        ],
        "external_packages": [
          "pyspark.sql.functions"
        ]
      },
      "suggested_patterns": [
        "pyspark.dropDuplicates",
        "pyspark.filter",
        "pyspark.withColumn",
        "pyspark.sql.functions.col",
        "pyspark.sql.functions.when",
        "pyspark.sql.functions.otherwise",
        "pyspark.Column.isNotNull"
      ]
    },
    {
      "function_name": "enrich_customer_data",
      "dependencies": {
        "internal_functions": [],
        "external_packages": [
          "pyspark.sql.functions"
        ]
      },
      "suggested_patterns": [
        "pyspark.groupBy",
        "pyspark.agg",
        "pyspark.join",
        "pyspark.withColumn",
        "pyspark.sql.functions.count",
        "pyspark.sql.functions.sum",
        "pyspark.sql.functions.avg",
        "pyspark.sql.functions.col",
        "pyspark.sql.functions.when",
        "pyspark.sql.functions.otherwise"
      ]
    },
    {
      "function_name": "generate_customer_insights",
      "dependencies": {
        "internal_functions": [],
        "external_packages": [
          "pyspark.sql.functions"
        ]
      },
      "suggested_patterns": [
        "pyspark.groupBy",
        "pyspark.agg",
        "pyspark.orderBy"
      ]
    },
    {
      "function_name": "save_results",
      "dependencies": {
        "internal_functions": [],
        "external_packages": [
          "pyspark.sql.DataFrame"
        ]
      },
      "suggested_patterns": [
        "pyspark.dataframe.coalesce",
        "pyspark.dataframe.write",
        "pyspark.dataframewriter.mode",
        "pyspark.dataframewriter.parquet",
        "pyspark.dataframewriter.csv",
        "pyspark.dataframewriter.json",
        "pyspark.dataframewriter.option"
      ]
    },
    {
      "function_name": "main",
      "dependencies": {
        "internal_functions": [
          "create_spark_session",
          "load_customer_data",
          "load_transaction_data",
          "clean_customer_data",
          "enrich_customer_data",
          "generate_customer_insights",
          "save_results"
        ],
        "external_packages": []
      },
      "suggested_patterns": [
        "pyspark.dataframe.show",
        "pyspark.dataframe.count",
        "pyspark.sparksession.stop"
      ]
    }
  ],
  "conversion_order": [
    "create_spark_session",
    "load_customer_data",
    "load_transaction_data",
    "enrich_customer_data",
    "generate_customer_insights",
    "save_results",
    "clean_customer_data",
    "main"
  ]
}